{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009d0129",
   "metadata": {},
   "source": [
    "First dataset : Data-Science and AI Jobs ‚Äì Indeed\n",
    "Source : Kaggle (Licence CC BY-SA 4.0)\n",
    "Nombre d‚Äôannonces : ~17 000\n",
    "Lien : https://www.kaggle.com/datasets/srivnaman/data-science-and-ai-jobsindeed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e4fe244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aper√ßu du Dataset 1 (5 premi√®res lignes):\n",
      "                                           Job Title  \\\n",
      "0  newArtificial intelligence security specialist...   \n",
      "1                              newEDA Tool Developer   \n",
      "2  Junior Project Engineer - Artificial Intellige...   \n",
      "3                                          Test Lead   \n",
      "4                                     + Lead Testers   \n",
      "\n",
      "                                         Description Location Date Company  \\\n",
      "0  An artificial intelligence (AI) specialist app...    India  NaT           \n",
      "1  We are seeking highly motivated individuals wi...    India  NaT           \n",
      "2  We are dedicated to providing quality and effe...  Unknown  NaT           \n",
      "3  Working on over 18 different browser/os/ mobil...  Unknown  NaT           \n",
      "4  Total Experience: 6- 8 yrs.\\nAutomation Testin...    India  NaT           \n",
      "\n",
      "  Salary URL Skills  \n",
      "0   None       None  \n",
      "1   None       None  \n",
      "2   None       None  \n",
      "3   None       None  \n",
      "4   None       None  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Traitement du Dataset 1\n",
    "def traiter_dataset1():\n",
    "    # Chargement\n",
    "    chemin = r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\Data-Science and AI Jobs ‚Äì Indeed\\DataScience and AI Jobs.csv\"\n",
    "    df = pd.read_csv(chemin)\n",
    "    \n",
    "    # V√©rification colonnes\n",
    "    colonnes_requises = [\"title\", \"location\", \"summary\", \"salary\"]\n",
    "    for col in colonnes_requises:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Colonne manquante : {col}\")\n",
    "    \n",
    "    # Transformation\n",
    "    df_final = pd.DataFrame({\n",
    "        'Job Title': df['title'],\n",
    "        'Description': df['summary'],\n",
    "        'Location': df['location'],\n",
    "        'Date': pd.NaT,\n",
    "        'Company': '',\n",
    "        'Salary': None,\n",
    "        'URL': '',\n",
    "        'Skills': None\n",
    "    })\n",
    "    \n",
    "    # Ajout pays\n",
    "    def detecter_pays(location):\n",
    "        loc = str(location).lower()\n",
    "        if any(mot in loc for mot in [\"india\", \"delhi\", \"mumbai\", \"hyderabad\", \"bengaluru\"]):\n",
    "            return \"India\"\n",
    "        if any(mot in loc for mot in [\"remote\", \"hybrid\"]):\n",
    "            return \"India\"\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    df_final['Location'] = df_final['Location'].apply(detecter_pays)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# 2. Ex√©cution et affichage\n",
    "dataset1 = traiter_dataset1()\n",
    "print(\"\\nAper√ßu du Dataset 1 (5 premi√®res lignes):\")\n",
    "print(dataset1.head())\n",
    "\n",
    "# 3. Stockage en m√©moire pour concat√©nation future\n",
    "datasets_traites = {\"dataset1\": dataset1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637865f6",
   "metadata": {},
   "source": [
    "second dataset : Data-Science, Data-Analyst & ML Jobs ‚Äì Indeed\n",
    "Source : Kaggle\n",
    "Lien : https://www.kaggle.com/datasets/mdwaquarazam/data-science-dataanalyst-and-ml-job-indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "717eb968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aper√ßu du Dataset 2 (5 premi√®res lignes):\n",
      "                                           Job Title  \\\n",
      "0  Technology Lead : Data Science I Machine Learning   \n",
      "1                Software Engineer, Machine Learning   \n",
      "2        Experienced Over The Shoulder Mechanic- C17   \n",
      "3                                     Data Scientist   \n",
      "4                                   Python Developer   \n",
      "\n",
      "                                         Description Location       Date  \\\n",
      "0  A day in the life of an Infoscion ‚Ä¢ As part of...    India 2022-06-26   \n",
      "1  2 years of relevant work experience in machine...    India 2022-06-26   \n",
      "2  This position will focus on supporting the Boe...    India 2022-07-23   \n",
      "3  Anchor ML development track in a client projec...    India 2022-06-26   \n",
      "4  Should have a decent understanding of the Mach...    India 2022-06-26   \n",
      "\n",
      "           Company Salary                                                URL  \\\n",
      "0  Infosys Limited   None  https://in.indeed.com/pagead/clk?mo=r&ad=-6NYl...   \n",
      "1           Google   None  https://in.indeed.com/pagead/clk?mo=r&ad=-6NYl...   \n",
      "2           BOEING   None  https://in.indeed.com/pagead/clk?mo=r&ad=-6NYl...   \n",
      "3  Infosys Limited   None  https://in.indeed.com/pagead/clk?mo=r&ad=-6NYl...   \n",
      "4  Infosys Limited   None  https://in.indeed.com/pagead/clk?mo=r&ad=-6NYl...   \n",
      "\n",
      "  Skills  \n",
      "0   None  \n",
      "1   None  \n",
      "2   None  \n",
      "3   None  \n",
      "4   None  \n",
      "\n",
      "R√©partition par pays:\n",
      "Location\n",
      "India      1580\n",
      "Unknown       3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import re\n",
    "\n",
    "# 1. Fonction de traitement pour le Dataset 2\n",
    "def traiter_dataset2():\n",
    "    # Chargement des donn√©es\n",
    "    chemin = r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\Data-Science, Data-Analyst & ML Jobs ‚Äì Indeed\\job_dataset.csv\"\n",
    "    df = pd.read_csv(chemin)\n",
    "    \n",
    "    # Nettoyage des noms de colonnes\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    # V√©rification des colonnes requises\n",
    "    colonnes_requises = [\"job_title\", \"company\", \"job_location\", \"job_summary\", \n",
    "                        \"post_date\", \"today\", \"job_salary\", \"job_url\"]\n",
    "    for col in colonnes_requises:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Colonne manquante : {col}\")\n",
    "    \n",
    "    # Conversion des dates\n",
    "    def convertir_date(post_date_str, reference_date):\n",
    "        if pd.isna(post_date_str):\n",
    "            return pd.NaT\n",
    "            \n",
    "        post_date_str = str(post_date_str)\n",
    "        \n",
    "        # Gestion des cas particuliers\n",
    "        if post_date_str in [\"Hiring ongoing\", \"PostedJust posted\", \"PostedToday\"]:\n",
    "            return reference_date\n",
    "        if post_date_str == \"PostedYesterday\":\n",
    "            return reference_date - timedelta(days=1)\n",
    "            \n",
    "        # Extraction du nombre de jours\n",
    "        jours = re.search(r'(\\d+)\\+? days? ago', post_date_str)\n",
    "        if jours:\n",
    "            return reference_date - timedelta(days=int(jours.group(1)))\n",
    "            \n",
    "        return pd.to_datetime(post_date_str, errors='coerce')\n",
    "    \n",
    "    reference_date = pd.to_datetime(df['today'].iloc[0])\n",
    "    df['date_calculee'] = df['post_date'].apply(lambda x: convertir_date(x, reference_date))\n",
    "    \n",
    "    # Cr√©ation du DataFrame standardis√©\n",
    "    df_final = pd.DataFrame({\n",
    "        'Job Title': df['job_title'],\n",
    "        'Description': df['job_summary'],\n",
    "        'Location': df['job_location'],\n",
    "        'Date': df['date_calculee'],\n",
    "        'Company': df['company'],\n",
    "        'Salary': None,\n",
    "        'URL': df['job_url'],\n",
    "        'Skills': None\n",
    "    })\n",
    "    \n",
    "    # Ajout de la colonne Country\n",
    "    def detecter_pays(location):\n",
    "        loc = str(location).lower()\n",
    "        return \"Unknown\" if \"unknown\" in loc else \"India\"\n",
    "    \n",
    "    df_final['Location'] = df_final['Location'].apply(detecter_pays)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# 2. Ex√©cution et affichage\n",
    "dataset2 = traiter_dataset2()\n",
    "print(\"\\nAper√ßu du Dataset 2 (5 premi√®res lignes):\")\n",
    "print(dataset2.head())\n",
    "print(\"\\nR√©partition par pays:\")\n",
    "print(dataset2['Location'].value_counts())\n",
    "\n",
    "# 3. Stockage en m√©moire\n",
    "datasets_traites[\"dataset2\"] = dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162feb5d",
   "metadata": {},
   "source": [
    "third dataset : Data-Science Jobs & Salaries ‚Äì Indeed\n",
    "Source : Kaggle\n",
    "Lien : https://www.kaggle.com/datasets/ritiksharma07/data-science-jobs-andsalaries-indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9222e189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aper√ßu du Dataset 3 (5 premi√®res lignes):\n",
      "                                           Job Title  \\\n",
      "0                                     Data Scientist   \n",
      "1  Senior Artificial Intelligence Researcher for ...   \n",
      "2                              Senior Data Scientist   \n",
      "3                                     Data Scientist   \n",
      "4                                     Data Scientist   \n",
      "\n",
      "                                         Description       Location  Date  \\\n",
      "0  The ideal candidate should be highly skilled i...  United States  <NA>   \n",
      "1  As a member of our research team, you will con...        Unknown  <NA>   \n",
      "2  Utilize machine learning techniques, algorithm...        Unknown  <NA>   \n",
      "3  Conduct ad-hoc analyses, build robust automate...        Unknown  <NA>   \n",
      "4  Develops and maintains analytics product proje...  United States  <NA>   \n",
      "\n",
      "                                          Company                      Salary  \\\n",
      "0                                     Robert Half  $120,000 - $140,000 a year   \n",
      "1  Johns Hopkins Applied Physics Laboratory (APL)                         NaN   \n",
      "2               Modern Technology Solutions, Inc.                         NaN   \n",
      "3                        Twitch Interactive, Inc.                         NaN   \n",
      "4               US Office of Personnel Management  $103,409 - $167,336 a year   \n",
      "\n",
      "                                                 URL Skills  \n",
      "0  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   None  \n",
      "1  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   None  \n",
      "2  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   None  \n",
      "3  https://www.indeed.com/rc/clk?jk=4593a41913786...   None  \n",
      "4  https://www.indeed.com/rc/clk?jk=d393197b7bc42...   None  \n",
      "\n",
      "R√©partition par pays:\n",
      "Location\n",
      "Unknown          138\n",
      "United States     62\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Statistiques des valeurs manquantes:\n",
      "- Dates: 200/200\n",
      "- Comp√©tences: 200/200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def traiter_dataset3():\n",
    "    # Chargement des donn√©es\n",
    "    chemin = r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\Data-Science Jobs & Salaries ‚Äì Indeed\\Indeed-Data Science Jobs List.csv\"\n",
    "    df = pd.read_csv(chemin)\n",
    "    \n",
    "    # V√©rification des colonnes requises\n",
    "    colonnes_requises = {'Job Title', 'Company', 'Location', 'Salary', \n",
    "                        'Short Description', 'Posted At', 'Job link'}\n",
    "    colonnes_manquantes = [col for col in colonnes_requises if col not in df.columns]\n",
    "    if colonnes_manquantes:\n",
    "        raise ValueError(f\"Colonnes manquantes : {colonnes_manquantes}\")\n",
    "    \n",
    "    # Cr√©ation du DataFrame standardis√©\n",
    "    df_final = pd.DataFrame({\n",
    "        'Job Title': df['Job Title'],\n",
    "        'Description': df['Short Description'],\n",
    "        'Location': df['Location'],\n",
    "        'Date': pd.NA,  # Dates non disponibles\n",
    "        'Company': df['Company'],\n",
    "        'Salary': df['Salary'],\n",
    "        'URL': df['Job link'],\n",
    "        'Skills': None  # Colonne √† remplir ult√©rieurement\n",
    "    })\n",
    "    \n",
    "    # Ajout de la colonne Country\n",
    "    def detecter_pays(location):\n",
    "        loc = str(location).lower()\n",
    "        if any(mot in loc for mot in [\"remote\", \"hybrid\", \"united states\"]):\n",
    "            return \"United States\"\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    df_final['Location'] = df_final['Location'].apply(detecter_pays)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# Ex√©cution et affichage\n",
    "dataset3 = traiter_dataset3()\n",
    "print(\"\\nAper√ßu du Dataset 3 (5 premi√®res lignes):\")\n",
    "print(dataset3.head())\n",
    "print(\"\\nR√©partition par pays:\")\n",
    "print(dataset3['Location'].value_counts())\n",
    "print(\"\\nStatistiques des valeurs manquantes:\")\n",
    "print(f\"- Dates: {dataset3['Date'].isna().sum()}/{len(dataset3)}\")\n",
    "print(f\"- Comp√©tences: {dataset3['Skills'].isna().sum()}/{len(dataset3)}\")\n",
    "\n",
    "# Stockage en m√©moire\n",
    "datasets_traites[\"dataset3\"] = dataset3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fafa493",
   "metadata": {},
   "source": [
    "fourth dataset (1) :‚Ä¢ ML Engineer Jobs ‚Äì Indeed\n",
    "Source : Kaggle\n",
    "Lien : https://www.kaggle.com/datasets/arnabk123/data-science-data-analystand-ml-jobs-from-indeed?select=ML+Engineer+jobs+%28Indeed%29.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b96e0fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aper√ßu du Dataset 4_1 (5 premi√®res lignes):\n",
      "     Job Title Description Location  Date  \\\n",
      "0  ML Engineer                India  <NA>   \n",
      "1  ML Engineer                India  <NA>   \n",
      "2  ML Engineer                India  <NA>   \n",
      "3  ML Engineer              Unknown  <NA>   \n",
      "4  ML Engineer                India  <NA>   \n",
      "\n",
      "                                           Company Salary URL  \\\n",
      "0                      LanceTech Solutions Pvt Ltd   <NA>       \n",
      "1                      LanceTech Solutions Pvt Ltd   <NA>       \n",
      "2                                           Google   <NA>       \n",
      "3                                 PMAM Corporation   <NA>       \n",
      "4  Mercedes-Benz Research and Development India...   <NA>       \n",
      "\n",
      "                            Skills  \n",
      "0                  Computer Vision  \n",
      "1                  Computer Vision  \n",
      "2        Deep Learning, Tensorflow  \n",
      "3                              NaN  \n",
      "4  Computer Vision, Neural Network  \n",
      "\n",
      "R√©partition par pays:\n",
      "Location\n",
      "India      141\n",
      "Unknown     21\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valeurs manquantes/par d√©faut:\n",
      "- Description: 162 / 162\n",
      "- Date: 162 / 162\n",
      "- URL: 162 / 162\n",
      "- Comp√©tences renseign√©es: 104 / 162\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def traiter_dataset4_1():\n",
    "    # Chargement des donn√©es\n",
    "    chemin = r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\ML Engineer Jobs ‚Äì Indeed\\ML Engineer jobs (Indeed).csv\"\n",
    "    df = pd.read_csv(chemin)\n",
    "    \n",
    "    # Normalisation des noms de colonnes\n",
    "    df.columns = df.columns.str.lower().str.strip()\n",
    "    \n",
    "    # V√©rification des colonnes requises\n",
    "    colonnes_requises = ['job title', 'company', 'region', 'skills required']\n",
    "    manquantes = [col for col in colonnes_requises if col not in df.columns]\n",
    "    if manquantes:\n",
    "        raise ValueError(f\"Colonnes manquantes : {manquantes}\")\n",
    "    \n",
    "    # Cr√©ation du DataFrame standardis√©\n",
    "    df_final = pd.DataFrame({\n",
    "        'Job Title': df['job title'],\n",
    "        'Description': '',  # Champ vide\n",
    "        'Location': df['region'],\n",
    "        'Date': pd.NA,  # Dates non disponibles\n",
    "        'Company': df['company'],\n",
    "        'Salary': pd.NA,  # Salaires non disponibles\n",
    "        'URL': '',  # URLs vides\n",
    "        'Skills': df['skills required']  # Seul dataset avec comp√©tences\n",
    "    })\n",
    "    \n",
    "    # D√©tection du pays\n",
    "    def detecter_pays(location):\n",
    "        loc = str(location).lower()\n",
    "        villes_indiennes = [\"agra\", \"bengaluru\", \"hyderabad\", \"noida\", \"mumbai\", \n",
    "                          \"delhi\", \"pune\", \"chennai\", \"kolkata\", \"jaipur\"]\n",
    "        \n",
    "        if \"india\" in loc or \"remote\" in loc or \"hybrid\" in loc:\n",
    "            return \"India\"\n",
    "        if any(ville in loc for ville in villes_indiennes):\n",
    "            return \"India\"\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    df_final['Location'] = df_final['Location'].apply(detecter_pays)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# Ex√©cution et affichage\n",
    "dataset4_1 = traiter_dataset4_1()\n",
    "print(\"\\nAper√ßu du Dataset 4_1 (5 premi√®res lignes):\")\n",
    "print(dataset4_1.head())\n",
    "print(\"\\nR√©partition par pays:\")\n",
    "print(dataset4_1['Location'].value_counts())\n",
    "print(\"\\nValeurs manquantes/par d√©faut:\")\n",
    "print(\"- Description:\", (dataset4_1['Description'] == '').sum(), \"/\", len(dataset4_1))\n",
    "print(\"- Date:\", dataset4_1['Date'].isna().sum(), \"/\", len(dataset4_1))\n",
    "print(\"- URL:\", (dataset4_1['URL'] == '').sum(), \"/\", len(dataset4_1))\n",
    "print(\"- Comp√©tences renseign√©es:\", dataset4_1['Skills'].notna().sum(), \"/\", len(dataset4_1))\n",
    "\n",
    "# Stockage en m√©moire\n",
    "datasets_traites[\"dataset4_1\"] = dataset4_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799975ee",
   "metadata": {},
   "source": [
    "fourth dataset (2) :‚Ä¢ Data Analyst jobs ‚Äì Indeed\n",
    "Source : Kaggle\n",
    "Lien : https://www.kaggle.com/datasets/arnabk123/data-science-data-analystand-ml-jobs-from-indeed?select=ML+Engineer+jobs+%28Indeed%29.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec4b02f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset 4_2 pr√™t \n",
      "      Job Title Description Location  Date                     Company Salary  \\\n",
      "0  Data analyst                India  <NA>                   BP Energy   <NA>   \n",
      "1  Data analyst                India  <NA>                     Arcadis   <NA>   \n",
      "2  Data analyst                India  <NA>  Scan Holdings Pvt. Limited   <NA>   \n",
      "3  Data analyst                India  <NA>                      BOEING   <NA>   \n",
      "4  Data analyst                India  <NA>     Boston Consulting Group   <NA>   \n",
      "\n",
      "  URL              Skills  \n",
      "0                     NaN  \n",
      "1      Big Data, Database  \n",
      "2                     NaN  \n",
      "3                     NaN  \n",
      "4                     NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def prepare_dataset4_2():\n",
    "    \"\"\"\n",
    "    Pr√©pare le dataset 4_2 en m√©moire, avec 'India' dans Location, sans colonne Country.\n",
    "    \"\"\"\n",
    "    chemin = r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\ML Engineer Jobs ‚Äì Indeed\\Data Analyst jobs (Indeed).csv\"\n",
    "\n",
    "    try:\n",
    "        # Chargement des donn√©es\n",
    "        df = pd.read_csv(chemin)\n",
    "        df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "        # V√©rification des colonnes requises\n",
    "        colonnes_requises = ['job title', 'company', 'skills required']\n",
    "        manquantes = [col for col in colonnes_requises if col not in df.columns]\n",
    "        if manquantes:\n",
    "            raise ValueError(f\"Colonnes manquantes : {manquantes}\")\n",
    "\n",
    "        # Cr√©ation du DataFrame standardis√©\n",
    "        df_final = pd.DataFrame({\n",
    "            'Job Title': df['job title'],\n",
    "            'Description': '',\n",
    "            'Location': 'India',  # Valeur fixe\n",
    "            'Date': pd.NA,\n",
    "            'Company': df['company'],\n",
    "            'Salary': pd.NA,\n",
    "            'URL': '',\n",
    "            'Skills': df['skills required']\n",
    "        })\n",
    "\n",
    "        print(\"‚úÖ Dataset 4_2 pr√™t \")\n",
    "        print(df_final.head())\n",
    "        return df_final\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur dans prepare_dataset4_2 : {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "datasets_traites[\"dataset4_2\"] = prepare_dataset4_2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef792ad",
   "metadata": {},
   "source": [
    "fourth dataset (3) :‚Ä¢ Data Analyst Jobs ‚Äì Indeed\n",
    "Source : Kaggle\n",
    "Lien : https://www.kaggle.com/datasets/arnabk123/data-science-data-analystand-ml-jobs-from-indeed?select=ML+Engineer+jobs+%28Indeed%29.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f3dcdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset 4_3 pr√™t (Location = 'India')\n",
      "      Job Title Description Location  Date                     Company Salary  \\\n",
      "0  Data analyst                India  <NA>                   BP Energy   <NA>   \n",
      "1  Data analyst                India  <NA>                     Arcadis   <NA>   \n",
      "2  Data analyst                India  <NA>  Scan Holdings Pvt. Limited   <NA>   \n",
      "3  Data analyst                India  <NA>                      BOEING   <NA>   \n",
      "4  Data analyst                India  <NA>     Boston Consulting Group   <NA>   \n",
      "\n",
      "  URL              Skills  \n",
      "0                     NaN  \n",
      "1      Big Data, Database  \n",
      "2                     NaN  \n",
      "3                     NaN  \n",
      "4                     NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def prepare_dataset4_3():\n",
    "    \"\"\"\n",
    "    Pr√©pare le dataset 4_3 en m√©moire avec 'Location' fix√© √† 'India', sans colonne 'Country'.\n",
    "    \"\"\"\n",
    "    chemin = r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\ML Engineer Jobs ‚Äì Indeed\\Data Analyst jobs (Indeed).csv\"\n",
    "\n",
    "    try:\n",
    "        # Chargement des donn√©es\n",
    "        df = pd.read_csv(chemin)\n",
    "        df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "        # V√©rification des colonnes requises\n",
    "        colonnes_requises = ['job title', 'company', 'skills required']\n",
    "        manquantes = [col for col in colonnes_requises if col not in df.columns]\n",
    "        if manquantes:\n",
    "            raise ValueError(f\"Colonnes manquantes : {manquantes}\")\n",
    "\n",
    "        # Cr√©ation du DataFrame standardis√©\n",
    "        df_final = pd.DataFrame({\n",
    "            'Job Title': df['job title'],\n",
    "            'Description': '',\n",
    "            'Location': 'India',  # Valeur fixe\n",
    "            'Date': pd.NA,\n",
    "            'Company': df['company'],\n",
    "            'Salary': pd.NA,\n",
    "            'URL': '',\n",
    "            'Skills': df['skills required']\n",
    "        })\n",
    "\n",
    "        print(\"‚úÖ Dataset 4_3 pr√™t (Location = 'India')\")\n",
    "        print(df_final.head())\n",
    "        return df_final\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur dans prepare_dataset4_3 : {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "datasets_traites[\"dataset4_3\"] = prepare_dataset4_3()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be108ee7",
   "metadata": {},
   "source": [
    "fifth dataset :Data-Science Job Postings & Skills (servira de lexique open source pour\n",
    "l‚Äôextraction des comp√©tences)\n",
    "Source : Kaggle\n",
    "Lien : https://www.kaggle.com/datasets/asaniczka/data-science-job-postingsand-skills/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cec00dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset 5 transform√© en m√©moire (Location ‚Üí Country)\n",
      "üîé Offres trait√©es: 12217\n",
      "üåç R√©partition par pays :\n",
      "Location\n",
      "United States     10163\n",
      "United Kingdom     1117\n",
      "Canada              634\n",
      "Australia           175\n",
      "Unknown             114\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def format_date_to_iso(date_str):\n",
    "    \"\"\"Convertit une date en format ISO (YYYY-MM-DD).\"\"\"\n",
    "    if pd.isna(date_str) or str(date_str).strip() == '':\n",
    "        return None\n",
    "    try:\n",
    "        dt = pd.to_datetime(date_str, errors='coerce')\n",
    "        return dt.strftime('%Y-%m-%d') if not pd.isna(dt) else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def map_location_to_country(location):\n",
    "    \"\"\"Mappe un champ location √† un pays.\"\"\"\n",
    "    if pd.isna(location):\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    loc = location.lower()\n",
    "\n",
    "    # Canada\n",
    "    if any(x in loc for x in [\"canada\", \"ontario\", \"quebec\", \"british columbia\", \"nova scotia\",\n",
    "                              \"alberta\", \"saskatchewan\", \"manitoba\", \"new brunswick\", \"newfoundland\",\n",
    "                              \"prince edward island\"]):\n",
    "        return \"Canada\"\n",
    "    \n",
    "    # United Kingdom\n",
    "    if any(x in loc for x in [\"england\", \"united kingdom\", \"scotland\", \"wales\",\n",
    "                              \"northern ireland\", \"london area\"]):\n",
    "        return \"United Kingdom\"\n",
    "    \n",
    "    # Australia\n",
    "    if any(x in loc for x in [\"australia\", \"queensland\", \"victoria\", \"new south wales\",\n",
    "                              \"australian capital territory\", \"western australia\", \"darwin\"]):\n",
    "        return \"Australia\"\n",
    "\n",
    "    # Mexico\n",
    "    if any(x in loc for x in [\"mexico\", \"baja california\"]):\n",
    "        return \"Mexico\"\n",
    "\n",
    "    # Italy\n",
    "    if any(x in loc for x in [\"italy\", \"lombardy\"]):\n",
    "        return \"Italy\"\n",
    "\n",
    "    # United States\n",
    "    us_states = [\"united states\", \"ca\", \"ny\", \"tx\", \"fl\", \"wa\", \"pa\", \"va\", \"nc\", \"oh\", \"il\", \"nj\", \"az\", \"ga\",\n",
    "                 \"or\", \"co\", \"mi\", \"mn\", \"wi\", \"sc\", \"in\", \"mo\", \"ok\", \"ky\", \"al\", \"tn\", \"la\", \"nv\", \"ar\", \"ut\",\n",
    "                 \"nm\", \"ia\", \"ks\", \"ct\", \"ma\", \"ri\", \"vt\", \"nh\", \"me\", \"mt\", \"nd\", \"sd\", \"ne\", \"id\", \"wv\", \"wy\",\n",
    "                 \"ak\", \"hi\"]\n",
    "    if any(state in loc for state in us_states):\n",
    "        return \"United States\"\n",
    "    \n",
    "    return \"Unknown\"\n",
    "\n",
    "def transform_dataset5_memory(input_path1, input_path2):\n",
    "    try:\n",
    "        # Chargement des fichiers\n",
    "        df_main = pd.read_csv(input_path1)\n",
    "        df_skills = pd.read_csv(input_path2)\n",
    "        \n",
    "        # Format de date\n",
    "        df_main['formatted_date'] = df_main['last_processed_time'].apply(format_date_to_iso)\n",
    "        \n",
    "        # Agr√©gation des comp√©tences par job_link\n",
    "        skills_agg = df_skills.groupby('job_link')['job_skills'].agg(\n",
    "            lambda x: ', '.join([str(s) for s in x if pd.notna(s)])\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Fusion des deux dataframes\n",
    "        df_final = pd.merge(df_main, skills_agg, on='job_link', how='left')\n",
    "        df_final.columns = df_final.columns.str.lower().str.strip()\n",
    "        \n",
    "        # Construction du DataFrame final au format standard\n",
    "        new_df = pd.DataFrame({\n",
    "            'Job Title': df_final['job_title'],\n",
    "            'Description': '',\n",
    "            'Location': df_final['job_location'],\n",
    "            'Date': df_final['formatted_date'],\n",
    "            'Company': df_final['company'],\n",
    "            'Salary': np.nan,\n",
    "            'URL': df_final['job_link'],\n",
    "            'Skills': df_final['job_skills']\n",
    "        })\n",
    "\n",
    "        # Remplacement de 'Location' par le pays d√©tect√©\n",
    "        new_df['Location'] = new_df['Location'].apply(map_location_to_country)\n",
    "\n",
    "        print(\"‚úÖ Dataset 5 transform√© en m√©moire (Location ‚Üí Country)\")\n",
    "        print(f\"üîé Offres trait√©es: {len(new_df)}\")\n",
    "        print(f\"üåç R√©partition par pays :\\n{new_df['Location'].value_counts().head()}\")\n",
    "\n",
    "        return new_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur dans transform_dataset5_memory : {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "datasets_traites[\"dataset5\"] = transform_dataset5_memory(\n",
    "    r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\Data-Science Job Postings & Skills\\job_postings.csv\",\n",
    "    r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\Data-Science Job Postings & Skills\\job_skills.csv\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "825ba0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset 6 pr√©par√© en m√©moire\n",
      "Location     \n",
      "United States    4631\n",
      "Puerto Rico        46\n",
      "Unknown            22\n",
      "Canada              5\n",
      "Australia           4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def prepare_dataset6():\n",
    "    input_file = r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\scrapped_jobs_api.csv\"\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    def extract_country(location):\n",
    "        if pd.isna(location):\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        loc = location.lower()\n",
    "\n",
    "        if \"puerto rico\" in loc or \"pr\" in loc:\n",
    "            return \"Puerto Rico\"\n",
    "        \n",
    "        if any(x in loc for x in [\"canada\", \"ontario\", \"british columbia\", \"alberta\", \"quebec\", \"nova scotia\", \"manitoba\", \"new brunswick\"]):\n",
    "            return \"Canada\"\n",
    "        \n",
    "        if any(x in loc for x in [\"united kingdom\", \"england\", \"scotland\", \"wales\", \"london\", \"northern ireland\"]):\n",
    "            return \"United Kingdom\"\n",
    "        \n",
    "        if any(x in loc for x in [\"australia\", \"queensland\", \"victoria\", \"new south wales\", \"australian capital territory\", \"nsw\", \"melbourne\", \"sydney\"]):\n",
    "            return \"Australia\"\n",
    "        \n",
    "        if any(x in loc for x in [\"mexico\", \"baja california\"]):\n",
    "            return \"Mexico\"\n",
    "\n",
    "        if any(x in loc for x in [\n",
    "            \"united states\", \"usa\", \"al\", \"ak\", \"az\", \"ar\", \"ca\", \"co\", \"ct\", \"dc\", \"de\", \"fl\", \"ga\", \"hi\", \"id\", \"il\", \"in\", \"ia\",\n",
    "            \"ks\", \"ky\", \"la\", \"me\", \"md\", \"ma\", \"mi\", \"mn\", \"ms\", \"mo\", \"mt\", \"ne\", \"nv\", \"nh\", \"nj\", \"nm\", \"ny\", \"nc\", \"nd\", \"oh\",\n",
    "            \"ok\", \"or\", \"pa\", \"ri\", \"sc\", \"sd\", \"tn\", \"tx\", \"ut\", \"vt\", \"va\", \"wa\", \"wv\", \"wi\", \"wy\"\n",
    "        ]) or \",\" in loc:\n",
    "            return \"United States\"\n",
    "\n",
    "        return \"Unknown\"\n",
    "\n",
    "    # Appliquer la d√©tection de pays\n",
    "    df['Location'] = df['Location'].apply(extract_country)\n",
    "\n",
    "    # Cr√©ation du DataFrame final au format commun\n",
    "    df_final = pd.DataFrame({\n",
    "        'Job Title': df['Title'] if 'Title' in df.columns else np.nan,\n",
    "        'Description': '',\n",
    "        'Location': df['Location'],\n",
    "        'Date': df['DatePosted'] if 'DatePosted' in df.columns else np.nan,\n",
    "        'Company': df['Company'] if 'Company' in df.columns else np.nan,\n",
    "        'Salary': np.nan,\n",
    "        'URL': df['URL'] if 'URL' in df.columns else np.nan,\n",
    "        'Skills': ''\n",
    "    })\n",
    "\n",
    "    print(\"‚úÖ Dataset 6 pr√©par√© en m√©moire\")\n",
    "    print(df_final[['Location']].value_counts().head())\n",
    "\n",
    "    return df_final\n",
    "datasets_traites[\"dataset6\"] = prepare_dataset6()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ff892",
   "metadata": {},
   "source": [
    "join dataset et traitement du salary et push dans mongodb compass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eeb4999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Fusion des datasets en m√©moire...\n",
      "üßπ Normalisation des salaires...\n",
      "‚úÖ Salaires normalis√©s\n",
      "üïí Normalisation des dates au format 'DD-MM-YYYY'...\n",
      "‚úÖ Dates normalis√©es\n",
      "üìä Aper√ßu du jeu de donn√©es fusionn√© :\n",
      "                                           Job Title  \\\n",
      "0  newArtificial intelligence security specialist...   \n",
      "1                              newEDA Tool Developer   \n",
      "2  Junior Project Engineer - Artificial Intellige...   \n",
      "3                                          Test Lead   \n",
      "4                                     + Lead Testers   \n",
      "\n",
      "                                         Description Location Date Company  \\\n",
      "0  An artificial intelligence (AI) specialist app...    India  NaN           \n",
      "1  We are seeking highly motivated individuals wi...    India  NaN           \n",
      "2  We are dedicated to providing quality and effe...  Unknown  NaN           \n",
      "3  Working on over 18 different browser/os/ mobil...  Unknown  NaN           \n",
      "4  Total Experience: 6- 8 yrs.\\nAutomation Testin...    India  NaN           \n",
      "\n",
      "   Salary URL Skills  \n",
      "0     NaN       None  \n",
      "1     NaN       None  \n",
      "2     NaN       None  \n",
      "3     NaN       None  \n",
      "4     NaN       None  \n",
      "\n",
      "üì° Connexion √† MongoDB Compass...\n",
      "‚úÖ 21280 documents ins√©r√©s avec succ√®s dans MongoDB !\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# --- 1. Normalisation des salaires ---\n",
    "def normalize_salary(s):\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    s_clean = s.replace(',', '').replace('CA$', '').replace('USD', '').replace('$', '').strip()\n",
    "    match = re.findall(r'(\\d+\\.?\\d*)\\s*[-‚Äì]?\\s*(\\d+\\.?\\d*)?\\s*(a year|yearly|a month|a week|an hour|a day|hr|mo|yr|hour|monthly|daily)?', s_clean.lower())\n",
    "    if not match:\n",
    "        return np.nan\n",
    "    try:\n",
    "        min_val = float(match[0][0])\n",
    "        max_val = float(match[0][1]) if match[0][1] else min_val\n",
    "        avg_val = (min_val + max_val) / 2\n",
    "        period = match[0][2]\n",
    "        if \"hour\" in period or \"hr\" in period:\n",
    "            return avg_val * 40 * 52\n",
    "        elif \"week\" in period:\n",
    "            return avg_val * 52\n",
    "        elif \"month\" in period or \"mo\" in period:\n",
    "            return avg_val * 12\n",
    "        elif \"day\" in period or \"daily\" in period:\n",
    "            return avg_val * 5 * 52\n",
    "        else:\n",
    "            return avg_val\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# --- 2. Pipeline principal ---\n",
    "def pipeline_in_memory(datasets_dict):\n",
    "    print(\"üîÑ Fusion des datasets en m√©moire...\")\n",
    "    df_final = pd.concat(datasets_dict.values(), ignore_index=True)\n",
    "\n",
    "    print(\"üßπ Normalisation des salaires...\")\n",
    "    df_final[\"Salary\"] = df_final[\"Salary\"].apply(normalize_salary)\n",
    "    print(\"‚úÖ Salaires normalis√©s\")\n",
    "\n",
    "    # üîß Normalisation des dates\n",
    "    print(\"üïí Normalisation des dates au format 'DD-MM-YYYY'...\")\n",
    "    df_final[\"Date\"] = pd.to_datetime(df_final[\"Date\"], errors='coerce').dt.strftime('%d-%m-%Y')\n",
    "    print(\"‚úÖ Dates normalis√©es\")\n",
    "    print(\"üìä Aper√ßu du jeu de donn√©es fusionn√© :\")\n",
    "    print(df_final.head())\n",
    "\n",
    "    print(\"\\nüì° Connexion √† MongoDB Compass...\")\n",
    "    try:\n",
    "        client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "        db = client[\"job_database\"]\n",
    "        collection = db[\"job_offers\"]\n",
    "\n",
    "        # Nettoyer la collection avant insertion (optionnel mais conseill√©)\n",
    "        collection.delete_many({})\n",
    "\n",
    "        # Insertion\n",
    "        records = df_final.to_dict(orient=\"records\")\n",
    "        collection.insert_many(records)\n",
    "        print(f\"‚úÖ {len(records)} documents ins√©r√©s avec succ√®s dans MongoDB !\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de l'insertion dans MongoDB: {e}\")\n",
    "\n",
    "# --- 3. Lancement ---\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline_in_memory(datasets_traites)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3e7d1",
   "metadata": {},
   "source": [
    "job skills dataset (relational database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "01f754b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final CSV saved as 'job_data_cleaned_final.csv'\n",
      "‚úÖ Donn√©es ins√©r√©es dans MongoDB Compass (collection: job_offers)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load CSVs\n",
    "company_df = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\skill job dataset relationnel\\company_dim.csv\")\n",
    "job_df = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\skill job dataset relationnel\\job_postings_fact.csv\")\n",
    "skills_df = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\skill job dataset relationnel\\skills_dim.csv\")\n",
    "skills_job_df = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\web scraping\\Last project\\datasets\\skill job dataset relationnel\\skills_job_dim.csv\")\n",
    "\n",
    "# Merge job postings with companies\n",
    "job_with_company = pd.merge(job_df, company_df, on=\"company_id\", how=\"left\")\n",
    "\n",
    "# Merge skills with their IDs\n",
    "skills_full = pd.merge(skills_job_df, skills_df, on=\"skill_id\", how=\"left\")\n",
    "\n",
    "# Merge everything by job_id\n",
    "final_df = pd.merge(job_with_company, skills_full, on=\"job_id\", how=\"left\")\n",
    "\n",
    "# Group by job_id and aggregate\n",
    "cleaned_df = final_df.groupby(\"job_id\").agg({\n",
    "    \"job_title_short\": \"first\",\n",
    "    \"job_posted_date\": \"first\",\n",
    "    \"job_country\": \"first\",\n",
    "    \"name\": \"first\",\n",
    "    \"salary_year_avg\": \"first\",\n",
    "    \"link\": \"first\",\n",
    "    \"skills\": lambda x: ', '.join(sorted(set(filter(pd.notna, x))))\n",
    "}).reset_index()\n",
    "\n",
    "# Format date\n",
    "cleaned_df[\"Date\"] = pd.to_datetime(cleaned_df[\"job_posted_date\"], errors='coerce').dt.strftime('%d-%m-%Y')\n",
    "\n",
    "# Add Description as NaN\n",
    "cleaned_df[\"Description\"] = pd.NA\n",
    "\n",
    "# Rename columns\n",
    "cleaned_df = cleaned_df.rename(columns={\n",
    "    \"job_title_short\": \"Job Title\",\n",
    "    \"job_country\": \"Location\",\n",
    "    \"name\": \"Company\",\n",
    "    \"salary_year_avg\": \"Salary\",\n",
    "    \"link\": \"URL\",\n",
    "    \"skills\": \"Skills\"\n",
    "})\n",
    "\n",
    "# Reorder columns\n",
    "final_columns = [\"Job Title\", \"Description\", \"Location\", \"Date\", \"Company\", \"Salary\", \"URL\", \"Skills\"]\n",
    "cleaned_df = cleaned_df[final_columns]\n",
    "\n",
    "# Save to CSV\n",
    "cleaned_df.to_csv(\"job_data_cleaned_final.csv\", index=False)\n",
    "print(\"‚úÖ Final CSV saved as 'job_data_cleaned_final.csv'\")\n",
    "\n",
    "# Push to MongoDB Compass\n",
    "try:\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"job_database\"]\n",
    "    collection = db[\"job_offers\"]\n",
    "\n",
    "    # Convert DataFrame to dictionary and insert into MongoDB\n",
    "    collection.insert_many(cleaned_df.to_dict(orient=\"records\"))\n",
    "    print(\"‚úÖ Donn√©es ins√©r√©es dans MongoDB Compass (collection: job_offers)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de l'insertion MongoDB : {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
